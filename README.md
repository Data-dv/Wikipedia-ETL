# Wikipedia-ETL

## Overwiew
In this project, I assumed the role of a data engineer for an EdTech StartUp in Lagos, Nigeria. I built a simple and robust data pipeline that extracts the information about the largest universities in the world from Wikipedia page, transforms and cleans the data, and then stores it in a database necessary for the use-case scenarios. Different tools, programming language and libraries was used to ensure the data is properly formated and inserted into the databases used, such as PostgreSQL and MySQL. A virtual enivronment and docker was used for local development of the data pipeline and development. Once the extracted data is transformed, cleaned and stored, a data visualization tool like Metabase was connected to create interactive charts and graphs giving visuals to business problems. Additionally, the stored data was analyzed using SQL queries on PgAdmin to extract meaningful insights and informations. These analyses, combined with the visualizations, will help generate actionable insights to enhance the goal of the EdTech company. The project focuses on building a scalable and reproducible system, ensuring that the process can be easily replicated. This end-to-end pipeline will provide valuable experience in data engineering, processing, visualization, and analysis.
